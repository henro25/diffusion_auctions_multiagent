"""
Generalized CLIP Alignment Analysis for Multi-Agent Diffusion Auctions

This script analyzes the alignment between generated images and agent prompts
using CLIP similarity scores. It works with any number of agents (2, 3, 5, 10, 20, etc.)
and processes images generated by generate_images.py.

The script stores alignment results in separate folders for each prompt,
similar to how images are organized.

Authors: Lillian Sun, Warren Zhu, Henry Huang
Academic Context: 4th Year research project on multi-winner auctions for generative AI
"""

import os
import sys
import json
from typing import Dict, List, Optional
import argparse

import torch
from PIL import Image
from tqdm import tqdm
from transformers import CLIPModel, CLIPProcessor

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


# Setup HuggingFace cache before importing models
def setup_hf_cache():
    """Setup HuggingFace cache to use local SSD instead of slow NFS."""
    if "HF_HOME" in os.environ:
        print(f"Using existing HF cache: {os.environ['HF_HOME']}")
        return

    user = os.environ.get("USER", os.environ.get("USERNAME", "user"))
    cache_paths = [
        f"/scratch/{user}/hf-cache",
        f"/tmp/{user}/hf-cache",
        f"/dev/shm/{user}/hf-cache",
        f"/var/tmp/{user}/hf-cache",
    ]

    cache_dir = None
    for path in cache_paths:
        parent = os.path.dirname(path)
        if os.access(parent, os.W_OK):
            cache_dir = path
            break

    if cache_dir:
        os.makedirs(cache_dir, exist_ok=True)
        os.makedirs(f"{cache_dir}/hub", exist_ok=True)
        os.makedirs(f"{cache_dir}/transformers", exist_ok=True)

        os.environ["HF_HOME"] = cache_dir
        os.environ["HF_HUB_CACHE"] = f"{cache_dir}/hub"
        os.environ["TRANSFORMERS_CACHE"] = f"{cache_dir}/transformers"

        print(f"Setup HF cache at: {cache_dir}")
    else:
        print("Warning: Could not find local SSD for cache, using default (may be slow)")


setup_hf_cache()

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")
DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float16
CACHE_DIR = os.environ.get("HF_HOME", os.path.expanduser("~/.cache/huggingface"))
CLIP_MODEL_ID = "openai/clip-vit-large-patch14-336"


def load_config(config_path: str) -> Dict:
    try:
        with open(config_path, "r") as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading config from {config_path}: {e}")
        sys.exit(1)


def load_prompts(prompts_path: str) -> List[Dict]:
    try:
        with open(prompts_path, "r") as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading prompts from {prompts_path}: {e}")
        return []


def get_clip_alignment_score(image_path: str, text: str, model: CLIPModel, processor: CLIPProcessor) -> float:
    try:
        img = Image.open(image_path).convert("RGB")
        inputs = processor(text=[text], images=[img], return_tensors="pt", padding=True, truncation=True)
        inputs = inputs.to(DEVICE)

        pixel_values = inputs["pixel_values"].to(DTYPE)
        input_ids = inputs["input_ids"]
        attention_mask = inputs.get("attention_mask", None)

        with torch.inference_mode():
            image_features = model.get_image_features(pixel_values=pixel_values)
            text_features = model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            similarity = (image_features @ text_features.T).squeeze()
            return float(similarity.item())
    except Exception as e:
        print(f"Error processing {image_path} with text '{text[:50]}...': {e}")
        return 0.0


def calculate_clip_quality(image_path: str, model: CLIPModel, processor: CLIPProcessor) -> float:
    try:
        with Image.open(image_path) as im:
            img = im.convert("RGB")
        inputs = processor(text=["High quality image.", "Low quality image."], images=[img], return_tensors="pt", padding=True).to(DEVICE)
        pixel_values = inputs["pixel_values"].to(model.dtype)
        input_ids = inputs["input_ids"]
        attention_mask = inputs.get("attention_mask", None)

        with torch.inference_mode():
            image_feats = model.get_image_features(pixel_values=pixel_values)
            text_feats = model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)
        image_feats = image_feats / image_feats.norm(dim=-1, keepdim=True)
        text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)
        logits = image_feats @ text_feats.T
        probs = logits.softmax(dim=-1)
        return float(probs[0, 0].item())
    except Exception as e:
        print(f"Error calculating quality for {image_path}: {e}")
        return 0.0


def construct_image_filename(prompt_idx: int, bids: List[float], sample_idx: int, num_agents: int) -> str:
    bid_str = "_".join([f"b{i+1}_{bid:.2f}" for i, bid in enumerate(bids)])
    return f"idx{prompt_idx:03d}_{bid_str}_s{sample_idx:02d}.png"


def construct_alignment_filename(prompt_idx: int, bids: List[float], sample_idx: int) -> str:
    bid_str = "_".join([f"{bid:.2f}" for bid in bids])
    return f"alignment_p{prompt_idx:03d}_b{bid_str}_s{sample_idx:02d}.json"


def process_image(image_path: str, prompt_data: Dict, prompt_idx: int, bids: List[float], sample_idx: int, num_agents: int, clip_model: CLIPModel, clip_processor: CLIPProcessor) -> Optional[Dict]:
    try:
        if not os.path.exists(image_path):
            return None

        base_score = get_clip_alignment_score(image_path, prompt_data["base_prompt"], clip_model, clip_processor)
        agent_scores = {}
        agent_prompts_dict = {}
        for i in range(1, num_agents + 1):
            agent_key = f"agent{i}_prompt"
            if agent_key in prompt_data:
                agent_prompts_dict[agent_key] = prompt_data[agent_key]
                agent_scores[f"agent{i}_alignment"] = get_clip_alignment_score(image_path, prompt_data[agent_key], clip_model, clip_processor)
            else:
                agent_prompts_dict[agent_key] = ""
                agent_scores[f"agent{i}_alignment"] = 0.0

        clip_quality_score = calculate_clip_quality(image_path, clip_model, clip_processor)
        total_welfare = sum(agent_scores[f"agent{i+1}_alignment"] * bids[i] for i in range(num_agents))
        weighted_alignment = total_welfare / sum(bids) if sum(bids) > 0 else 0.0
        filename = os.path.basename(image_path)

        return {
            "metadata": {"prompt_index": prompt_idx, "bids": bids, "sample_index": sample_idx, "image_path": image_path, "filename": filename},
            "prompts": {"base_prompt": prompt_data["base_prompt"], **agent_prompts_dict},
            "alignment_scores": {"base_alignment": base_score, **agent_scores},
            "quality_assessment": {"clip_quality": clip_quality_score},
            "welfare_metrics": {"weighted_alignment": weighted_alignment, "total_welfare": total_welfare},
        }
    except Exception as e:
        print(f"Error processing image {image_path}: {e}")
        return None


def main():
    parser = argparse.ArgumentParser(description="Analyze CLIP alignment for multi-agent generated images")
    parser.add_argument("--config", type=str, required=True, help="Path to configuration JSON file")
    args = parser.parse_args()

    config = load_config(args.config)
    num_agents = config.get("num_agents")
    prompts_path = config.get("prompts_path")
    images_dir = config.get("images_dir")
    output_dir = config.get("output_dir")
    num_samples_per_combination = config.get("num_samples_per_combination", 20)
    num_prompts_to_process = config.get("num_prompts_to_process", None)
    process_prompts_forward = config.get("process_prompts_forward", True)
    bidding_combinations = config.get("bidding_combinations", [])

    if not all([num_agents, prompts_path, images_dir, output_dir, bidding_combinations]):
        print("Error: Config missing required fields")
        sys.exit(1)

    bidding_combinations = [list(combo) for combo in bidding_combinations]
    print(f"\n=== {num_agents}-Agent CLIP Alignment Analysis ===")

    if not os.path.exists(prompts_path):
        print(f"Error: Prompts file not found at {prompts_path}")
        sys.exit(1)

    prompts = load_prompts(prompts_path)
    if not prompts:
        print("Error: Could not load prompts")
        sys.exit(1)

    num_items_to_process = num_prompts_to_process if num_prompts_to_process is not None else len(prompts)
    print(f"\nConfiguration: {num_agents} agents, {num_items_to_process} prompts, {len(bidding_combinations)} bid combos, {num_samples_per_combination} samples")
    print(f"Process: {'forward' if process_prompts_forward else 'backward'}")

    os.makedirs(output_dir, exist_ok=True)

    print("\nLoading CLIP model...")
    clip_model = CLIPModel.from_pretrained(CLIP_MODEL_ID, cache_dir=CACHE_DIR).to(dtype=DTYPE).to(DEVICE)
    clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_ID, cache_dir=CACHE_DIR)
    print("CLIP model loaded")

    processed_count = 0
    skipped_missing = 0
    skipped_existing = 0
    total_expected = num_items_to_process * len(bidding_combinations) * num_samples_per_combination

    prompt_indices = range(num_items_to_process) if process_prompts_forward else range(num_items_to_process - 1, -1, -1)

    with tqdm(total=total_expected, desc="Processing images") as pbar:
        for prompt_idx in prompt_indices:
            if prompt_idx >= len(prompts):
                pbar.update(len(bidding_combinations) * num_samples_per_combination)
                continue

            prompt_data = prompts[prompt_idx]
            prompt_output_dir = os.path.join(output_dir, f"prompt_{prompt_idx:03d}")
            os.makedirs(prompt_output_dir, exist_ok=True)

            for bids in bidding_combinations:
                for sample_idx in range(num_samples_per_combination):
                    image_filename = construct_image_filename(prompt_idx, bids, sample_idx, num_agents)
                    image_path = os.path.join(images_dir, f"prompt_{prompt_idx:03d}", image_filename)
                    alignment_filename = construct_alignment_filename(prompt_idx, bids, sample_idx)
                    output_path = os.path.join(prompt_output_dir, alignment_filename)

                    if os.path.exists(output_path):
                        skipped_existing += 1
                        pbar.update(1)
                        continue

                    if not os.path.exists(image_path):
                        skipped_missing += 1
                        pbar.update(1)
                        continue

                    result = process_image(image_path, prompt_data, prompt_idx, bids, sample_idx, num_agents, clip_model, clip_processor)
                    if result:
                        with open(output_path, "w") as f:
                            json.dump(result, f, indent=2)
                        processed_count += 1
                    pbar.update(1)

    print(f"\n=== Complete === Processed: {processed_count}, Skipped existing: {skipped_existing}, Skipped missing: {skipped_missing}")
    del clip_model
    del clip_processor
    torch.cuda.empty_cache()


if __name__ == "__main__":
    main()
