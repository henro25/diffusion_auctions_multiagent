"""
CLIP & VLM Alignment Analysis for 3-Agent Diffusion Auctions (Multi-GPU)

This script analyzes the alignment between generated images and agent prompts
using both CLIP similarity scores and Vision Language Model (VLM) quality assessment.
It processes images generated by generate_images_3_agent_multigpu.py and creates individual
JSON files for each prompt/bidding combination/sample using multiple GPUs for faster processing.

Authors: Lillian Sun, Warren Zhu, Henry Huang
Academic Context: 4th Year research project on multi-winner auctions for generative AI
"""

import os
import sys
import json
from typing import Dict, List, Optional
import argparse
import glob
from concurrent.futures import ThreadPoolExecutor, as_completed

import torch
from PIL import Image
from tqdm import tqdm
from transformers import CLIPModel, CLIPProcessor

# Add parent directory to path for VLM imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from vlm_quality_assessor import VLMQualityAssessor


# Setup HuggingFace cache before importing models
def setup_hf_cache():
    """Setup HuggingFace cache to use local SSD instead of slow NFS."""
    # Check if cache is already configured
    if "HF_HOME" in os.environ:
        print(f"Using existing HF cache: {os.environ['HF_HOME']}")
        return

    # Try to find a local SSD path
    user = os.environ.get("USER", os.environ.get("USERNAME", "user"))
    cache_paths = [
        f"/scratch/{user}/hf-cache",
        f"/tmp/{user}/hf-cache",
        f"/dev/shm/{user}/hf-cache",
        f"/var/tmp/{user}/hf-cache",
    ]

    cache_dir = None
    for path in cache_paths:
        parent = os.path.dirname(path)
        if os.access(parent, os.W_OK):
            cache_dir = path
            break

    if cache_dir:
        os.makedirs(cache_dir, exist_ok=True)
        os.makedirs(f"{cache_dir}/hub", exist_ok=True)
        os.makedirs(f"{cache_dir}/transformers", exist_ok=True)

        os.environ["HF_HOME"] = cache_dir
        os.environ["HF_HUB_CACHE"] = f"{cache_dir}/hub"
        os.environ["TRANSFORMERS_CACHE"] = f"{cache_dir}/transformers"

        print(f"Setup HF cache at: {cache_dir}")
    else:
        print(
            "Warning: Could not find local SSD for cache, using default (may be slow)"
        )


# Setup cache before importing heavy libraries
setup_hf_cache()

# Multi-GPU Configuration
USE_MULTI_GPU = True
GPU_INDICES = [0, 1, 2, 3]  # Specify which GPUs to use, or None for all available

# Configuration
DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float16

# Cache directory - use dynamic setup from helpers/setup_cache.sh
# This will be set by the cache setup script or fallback to user's local cache
CACHE_DIR = os.environ.get("HF_HOME", os.path.expanduser("~/.cache/huggingface"))

# Paths
IMAGES_DIR = "../images/images_3_agent_multigpu"
OUTPUT_DIR = "../alignment/alignment_3_agent_multigpu"
PROMPTS_PATH = "../prompts/prompts_3_agent.json"

# CLIP Model Configuration
CLIP_MODEL_ID = "openai/clip-vit-large-patch14-336"


class AnalysisTask:
    """Represents a single image analysis task"""

    def __init__(
        self,
        image_path: str,
        prompt_data: Dict,
        metadata: Dict,
        output_path: str,
        enable_vlm: bool = False,
        vlm_config: str = None,
    ):
        self.image_path = image_path
        self.prompt_data = prompt_data
        self.metadata = metadata
        self.output_path = output_path
        self.enable_vlm = enable_vlm
        self.vlm_config = vlm_config


class MultiGPUAnalyzer:
    """Manages multi-GPU alignment analysis"""

    def __init__(
        self,
        gpu_indices: List[int] = None,
        torch_dtype=torch.bfloat16,
    ):
        self.torch_dtype = torch_dtype
        self.clip_models = {}
        self.clip_processors = {}
        self.vlm_assessors = {}

        # Determine which GPUs to use
        total_gpus = torch.cuda.device_count()
        if total_gpus == 0:
            raise RuntimeError("No CUDA GPUs available")

        if gpu_indices is not None:
            # Validate specified GPU indices
            invalid_indices = [
                idx for idx in gpu_indices if idx >= total_gpus or idx < 0
            ]
            if invalid_indices:
                raise ValueError(
                    f"Invalid GPU indices {invalid_indices}. Available GPUs: 0-{total_gpus - 1}"
                )
            self.gpu_indices = gpu_indices
        else:
            # Use all available GPUs
            self.gpu_indices = list(range(total_gpus))

        self.num_gpus = len(self.gpu_indices)
        print(
            f"Initializing MultiGPUAnalyzer with {self.num_gpus} GPUs: {self.gpu_indices}"
        )

    def load_models(self, enable_vlm: bool = False, vlm_config: str = None):
        """Load CLIP models and VLM assessors on each GPU"""
        for gpu_id in self.gpu_indices:
            device = torch.device(f"cuda:{gpu_id}")
            print(f"Loading CLIP model on GPU {gpu_id}...")

            # Load CLIP model and processor
            clip_model = (
                CLIPModel.from_pretrained(CLIP_MODEL_ID, cache_dir=CACHE_DIR)
                .to(dtype=self.torch_dtype)
                .to(device)
            )
            clip_processor = CLIPProcessor.from_pretrained(
                CLIP_MODEL_ID, cache_dir=CACHE_DIR
            )

            self.clip_models[gpu_id] = clip_model
            self.clip_processors[gpu_id] = clip_processor

            # Load VLM assessor if enabled
            if enable_vlm:
                try:
                    print(f"Loading VLM assessor on GPU {gpu_id}...")
                    vlm_assessor = VLMQualityAssessor(config_path=vlm_config)
                    # Set device for VLM assessor
                    vlm_assessor.device = device
                    vlm_assessor.load_models()
                    self.vlm_assessors[gpu_id] = vlm_assessor
                    print(
                        f"VLM assessor loaded on GPU {gpu_id} with models: {vlm_assessor.loaded_models}"
                    )
                except Exception as e:
                    print(f"Warning: Failed to load VLM assessor on GPU {gpu_id}: {e}")
                    self.vlm_assessors[gpu_id] = None

            # Clear cache after loading
            torch.cuda.empty_cache()

    def get_clip_alignment_score(
        self, image_path: str, text: str, gpu_id: int
    ) -> float:
        """Calculate CLIP alignment score for a single image and text prompt."""
        device = torch.device(f"cuda:{gpu_id}")
        model = self.clip_models[gpu_id]
        processor = self.clip_processors[gpu_id]

        try:
            torch.cuda.set_device(device)
            img = Image.open(image_path).convert("RGB")

            # Keep the BatchEncoding; move to device
            inputs = processor(
                text=[text],
                images=[img],
                return_tensors="pt",
                padding=True,
                truncation=True,
            )
            inputs = inputs.to(device)  # preserves BatchEncoding type

            # Pull tensors explicitly; only cast image to float dtype
            pixel_values = inputs["pixel_values"].to(self.torch_dtype)
            input_ids = inputs["input_ids"]
            attention_mask = inputs.get("attention_mask", None)

            with torch.inference_mode():
                image_features = model.get_image_features(pixel_values=pixel_values)
                text_features = model.get_text_features(
                    input_ids=input_ids, attention_mask=attention_mask
                )

                image_features = image_features / image_features.norm(
                    dim=-1, keepdim=True
                )
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)

                similarity = (image_features @ text_features.T).squeeze()
                return float(similarity.item())

        except Exception as e:
            print(
                f"Error processing {image_path} with text '{text}' on GPU {gpu_id}: {e}"
            )
            return 0.0

    def calculate_clip_quality(self, image_path: str, gpu_id: int) -> float:
        """Calculate image quality score using CLIP."""
        device = torch.device(f"cuda:{gpu_id}")
        model = self.clip_models[gpu_id]
        processor = self.clip_processors[gpu_id]

        try:
            torch.cuda.set_device(device)
            # Open the image first
            with Image.open(image_path) as im:
                img = im.convert("RGB")

            # Build inputs, then move to device
            inputs = processor(
                text=["High quality image.", "Low quality image."],
                images=[img],
                return_tensors="pt",
                padding=True,
            ).to(device)

            # Pull tensors; cast only the image to model dtype
            pixel_values = inputs["pixel_values"].to(model.dtype)
            input_ids = inputs["input_ids"]
            attention_mask = inputs.get("attention_mask", None)

            with torch.inference_mode():
                image_feats = model.get_image_features(pixel_values=pixel_values)
                text_feats = model.get_text_features(
                    input_ids=input_ids, attention_mask=attention_mask
                )

            # Normalize
            image_feats = image_feats / image_feats.norm(dim=-1, keepdim=True)
            text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)

            # Probabilities over ["High quality", "Low quality"]
            logits = image_feats @ text_feats.T  # (1, 2)
            probs = logits.softmax(dim=-1)  # (1, 2)

            return float(probs[0, 0].item())

        except Exception as e:
            print(f"Error calculating quality for {image_path} on GPU {gpu_id}: {e}")
            return 0.0

    def analyze_single_image(self, task: AnalysisTask, gpu_id: int) -> Dict:
        """Analyze a single image on specified GPU"""
        device = torch.device(f"cuda:{gpu_id}")
        torch.cuda.set_device(device)

        try:
            # Calculate alignment scores
            base_score = self.get_clip_alignment_score(
                task.image_path, task.prompt_data["base_prompt"], gpu_id
            )

            agent1_score = self.get_clip_alignment_score(
                task.image_path, task.prompt_data["agent1_prompt"], gpu_id
            )

            agent2_score = self.get_clip_alignment_score(
                task.image_path, task.prompt_data["agent2_prompt"], gpu_id
            )

            agent3_score = self.get_clip_alignment_score(
                task.image_path, task.prompt_data["agent3_prompt"], gpu_id
            )

            # Calculate image quality (CLIP)
            clip_quality_score = self.calculate_clip_quality(task.image_path, gpu_id)

            # Calculate VLM quality assessment if enabled
            vlm_quality_result = None
            if (
                task.enable_vlm
                and gpu_id in self.vlm_assessors
                and self.vlm_assessors[gpu_id]
            ):
                try:
                    vlm_quality_result = self.vlm_assessors[gpu_id].assess_single_image(
                        task.image_path
                    )
                except Exception as e:
                    print(
                        f"VLM assessment failed for {task.image_path} on GPU {gpu_id}: {e}"
                    )
                    vlm_quality_result = None

            bids = task.metadata["bids"]

            # Create analysis result
            result = {
                "metadata": task.metadata,
                "prompts": {
                    "base_prompt": task.prompt_data["base_prompt"],
                    "agent1_prompt": task.prompt_data["agent1_prompt"],
                    "agent2_prompt": task.prompt_data["agent2_prompt"],
                    "agent3_prompt": task.prompt_data["agent3_prompt"],
                },
                "alignment_scores": {
                    "base_alignment": base_score,
                    "agent1_alignment": agent1_score,
                    "agent2_alignment": agent2_score,
                    "agent3_alignment": agent3_score,
                },
                "quality_assessment": {
                    "clip_quality": clip_quality_score,
                    "vlm_quality": vlm_quality_result,
                }
                if vlm_quality_result
                else {"clip_quality": clip_quality_score},
                "welfare_metrics": {
                    "weighted_alignment": (
                        agent1_score * bids[0]
                        + agent2_score * bids[1]
                        + agent3_score * bids[2]
                    )
                    / sum(bids)
                    if sum(bids) > 0
                    else 0,
                    "total_welfare": agent1_score * bids[0]
                    + agent2_score * bids[1]
                    + agent3_score * bids[2],
                },
                "processing_info": {"gpu_id": gpu_id},
            }

            # Save individual result
            with open(task.output_path, "w") as f:
                json.dump(result, f, indent=2)

            return {
                "success": True,
                "gpu_id": gpu_id,
                "output_path": task.output_path,
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "gpu_id": gpu_id,
                "image_path": task.image_path,
            }
        finally:
            # Clear GPU cache after analysis
            torch.cuda.empty_cache()

    def analyze_images_parallel(
        self,
        tasks: List[AnalysisTask],
    ) -> List[Dict]:
        """Analyze images in parallel across multiple GPUs"""
        print(f"Analyzing {len(tasks)} images using {self.num_gpus} GPUs")

        results = []
        successful_results = []

        with ThreadPoolExecutor(max_workers=self.num_gpus) as executor:
            # Submit tasks to GPUs in round-robin fashion
            future_to_task = {}

            for i, task in enumerate(tasks):
                gpu_id = self.gpu_indices[i % self.num_gpus]
                future = executor.submit(self.analyze_single_image, task, gpu_id)
                future_to_task[future] = (task, gpu_id)

            # Collect results with progress bar
            with tqdm(total=len(tasks), desc="Analyzing images") as pbar:
                for future in as_completed(future_to_task):
                    task, gpu_id = future_to_task[future]
                    try:
                        result = future.result()
                        results.append(result)

                        if result["success"]:
                            successful_results.append(result)
                            pbar.set_postfix(
                                {
                                    "GPU": gpu_id,
                                    "Success": len(successful_results),
                                    "Failed": len(results) - len(successful_results),
                                }
                            )
                        else:
                            pbar.set_postfix(
                                {
                                    "GPU": gpu_id,
                                    "Success": len(successful_results),
                                    "Failed": len(results) - len(successful_results),
                                    "Last Error": result.get("error", "Unknown")[:30],
                                }
                            )

                    except Exception as exc:
                        print(f"Task generated an exception: {exc}")
                        results.append(
                            {"success": False, "error": str(exc), "gpu_id": gpu_id}
                        )

                    pbar.update(1)

        print(
            f"\nCompleted: {len(successful_results)} successful, {len(results) - len(successful_results)} failed"
        )
        return successful_results

    def cleanup(self):
        """Cleanup GPU memory and models"""
        print("Cleaning up GPU memory...")
        for gpu_id in self.gpu_indices:
            if gpu_id in self.clip_models:
                del self.clip_models[gpu_id]
            if gpu_id in self.clip_processors:
                del self.clip_processors[gpu_id]
            if gpu_id in self.vlm_assessors and self.vlm_assessors[gpu_id]:
                self.vlm_assessors[gpu_id].cleanup()
                del self.vlm_assessors[gpu_id]
        torch.cuda.empty_cache()


def parse_image_filename(filename: str) -> Optional[Dict]:
    """Parse image filename to extract metadata.

    Expected format: idx{prompt_index:03d}_b1_{bid1:.2f}_b2_{bid2:.2f}_b3_{bid3:.2f}_s{sample_idx:02d}.png
    """
    try:
        # Remove extension
        name = filename.replace(".png", "")

        # Split by underscores
        parts = name.split("_")

        # Extract components
        prompt_idx = int(parts[0].replace("idx", ""))
        bid1 = float(parts[2])
        bid2 = float(parts[4])
        bid3 = float(parts[6])
        sample_idx = int(parts[7].replace("s", ""))

        return {
            "prompt_index": prompt_idx,
            "bids": [bid1, bid2, bid3],
            "sample_index": sample_idx,
        }
    except Exception as e:
        print(f"Error parsing filename {filename}: {e}")
        return None


def load_prompts(prompts_path: str) -> List[Dict]:
    """Load prompts from JSON file."""
    try:
        with open(prompts_path, "r") as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading prompts from {prompts_path}: {e}")
        return []


def main():
    """Main analysis function."""
    parser = argparse.ArgumentParser(
        description="Analyze CLIP alignment and VLM quality for 3-agent generated images using multi-GPU"
    )
    parser.add_argument(
        "--prompt_index", type=int, help="Specific prompt index to process (optional)"
    )
    parser.add_argument(
        "--bid_combination",
        type=str,
        help="Specific bid combination like '1.0,0.0,0.0' (optional)",
    )
    parser.add_argument(
        "--sample_index", type=int, help="Specific sample index to process (optional)"
    )
    parser.add_argument(
        "--enable_vlm",
        action="store_true",
        help="Enable VLM quality assessment (requires VLM models)",
    )
    parser.add_argument(
        "--vlm_config",
        type=str,
        default="../config/vlm_config.json",
        help="Path to VLM configuration file",
    )
    parser.add_argument(
        "--gpu_indices",
        type=str,
        default=None,
        help="Comma-separated list of GPU indices to use (e.g., '0,1,2,3')",
    )
    args = parser.parse_args()

    # Parse GPU indices
    gpu_indices = GPU_INDICES
    if args.gpu_indices:
        gpu_indices = [int(x.strip()) for x in args.gpu_indices.split(",")]

    # Create output directory
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"Output directory: {OUTPUT_DIR}")

    # Load prompts
    prompts = load_prompts(PROMPTS_PATH)
    if not prompts:
        print("Error: Could not load prompts")
        return

    print(f"Loaded {len(prompts)} prompts")

    # Initialize multi-GPU analyzer
    if USE_MULTI_GPU and torch.cuda.is_available():
        analyzer = MultiGPUAnalyzer(gpu_indices=gpu_indices, torch_dtype=DTYPE)
        analyzer.load_models(enable_vlm=args.enable_vlm, vlm_config=args.vlm_config)
    else:
        print("Multi-GPU not available or disabled, falling back to single GPU")
        # Fallback to single GPU analysis (could implement this if needed)
        return

    # Find all image files
    image_pattern = os.path.join(IMAGES_DIR, "**", "*.png")
    image_paths = glob.glob(image_pattern, recursive=True)
    print(f"Found {len(image_paths)} images to process")

    if not image_paths:
        print(f"Warning: No images found in {IMAGES_DIR}")
        return

    # Create analysis tasks
    tasks = []
    for image_path in image_paths:
        try:
            # Parse filename to get metadata
            filename = os.path.basename(image_path)
            metadata = parse_image_filename(filename)

            if not metadata:
                print(f"Skipping {filename} - could not parse")
                continue

            prompt_idx = metadata["prompt_index"]
            bids = metadata["bids"]
            sample_idx = metadata["sample_index"]

            # Apply filters if specified
            if args.prompt_index is not None and prompt_idx != args.prompt_index:
                continue
            if args.bid_combination is not None:
                target_bids = [float(x) for x in args.bid_combination.split(",")]
                if not all(abs(a - b) < 0.01 for a, b in zip(bids, target_bids)):
                    continue
            if args.sample_index is not None and sample_idx != args.sample_index:
                continue

            # Get corresponding prompt data
            if prompt_idx >= len(prompts):
                print(f"Prompt index {prompt_idx} out of range, skipping")
                continue

            prompt_data = prompts[prompt_idx]

            # Create output path
            output_filename = f"alignment_p{prompt_idx:03d}_b{bids[0]:.2f}_{bids[1]:.2f}_{bids[2]:.2f}_s{sample_idx:02d}.json"
            output_path = os.path.join(OUTPUT_DIR, output_filename)

            # Skip if output already exists
            if os.path.exists(output_path):
                continue

            # Add metadata to include image path and filename
            metadata.update(
                {
                    "image_path": image_path,
                    "filename": filename,
                }
            )

            # Create analysis task
            task = AnalysisTask(
                image_path=image_path,
                prompt_data=prompt_data,
                metadata=metadata,
                output_path=output_path,
                enable_vlm=args.enable_vlm,
                vlm_config=args.vlm_config,
            )
            tasks.append(task)

        except Exception as e:
            print(f"Error creating task for {image_path}: {e}")
            continue

    if not tasks:
        print("No tasks to process")
        return

    print(f"Created {len(tasks)} analysis tasks")

    # Process images in parallel
    results = analyzer.analyze_images_parallel(tasks)

    print(f"Successfully processed {len(results)} images")
    print(f"Results saved to: {OUTPUT_DIR}")

    # Cleanup
    analyzer.cleanup()
    print("Analysis complete!")


if __name__ == "__main__":
    main()
